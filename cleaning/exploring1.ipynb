{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b12ba3b",
   "metadata": {},
   "source": [
    "# Attemp number 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e75417",
   "metadata": {},
   "source": [
    "When i first wrote in cleaning.py I used `script_path = Path(__file__).resolve().parent` which work well in a .py as `__file__` showed the exact location of the current file. However, inside the notebook the variable `__file__` doesnt exist, to continue looking for the file path I decided to use `.cwd` in the `data_path_cleaned = script_path / ... /` in notebook as this shows the same result while keeping `__file__` in cleaning.py as it is more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dd70ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 16)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "script_path = Path.cwd()\n",
    "data_path_cleaned = script_path.parent / \"data\" / \"games_cleaned.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path_cleaned, encoding=\"utf-8\")\n",
    "\n",
    "# Cheking for AppID duplicates\n",
    "duplicates = df[df.duplicated('AppID', keep=False)]\n",
    "print(duplicates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "339cc9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2063, 16)\n"
     ]
    }
   ],
   "source": [
    "# Checking for Name duplicates\n",
    "duplicates = df[df.duplicated('Name', keep=False)]\n",
    "print(duplicates.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006b63fa",
   "metadata": {},
   "source": [
    "As shown earlier, all `AppID` values were unique; however, there were 2,079 duplicate entries based on `Name`. To analyze the dataset effectively, we need a clear metric to determine which duplicates to keep and which to discard. In this project, I aim to focus on user feedback while preserving enough relevance to ensure the data remains valuable. Therefore, I will use the number of `Positive` and `Negative` reviews, the `Estimated number` of owners, and the game's `Release date` as criteria for filtering duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0959c716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Average owners  Positive ratio Release date\n",
      "111100             NaN             1.0   2025-04-18\n",
      "111176             NaN             1.0   2025-04-18\n",
      "111272             NaN             1.0   2025-04-18\n",
      "111419             NaN             1.0   2025-04-18\n",
      "110959             NaN             1.0   2025-04-17\n",
      "111056             NaN             1.0   2025-04-17\n",
      "111332             NaN             1.0   2025-04-17\n",
      "111390             NaN             1.0   2025-04-17\n",
      "111416             NaN             1.0   2025-04-17\n",
      "110975             NaN             1.0   2025-04-16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data types conversion\n",
    "df['Release date'] = pd.to_datetime(df['Release date'], errors='coerce')\n",
    "df['Positive'] = pd.to_numeric(df['Positive'], errors='coerce')\n",
    "df['Negative'] = pd.to_numeric(df['Negative'], errors='coerce')\n",
    "df['Average owners'] = pd.to_numeric(df['Estimated owners'], errors='coerce')\n",
    "\n",
    "df['Positive ratio'] = df['Positive'] / (df['Positive'] + df['Negative']).replace(0, np.nan)\n",
    "\n",
    "df_sorted = df.sort_values(\n",
    "    by=['Average owners', 'Positive ratio', 'Release date'],\n",
    "    ascending=[False, False, False]\n",
    ")\n",
    "\n",
    "print(df_sorted[['Average owners', 'Positive ratio', 'Release date']].head(10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c2193",
   "metadata": {},
   "source": [
    "The `Estimated owners` column was showing a lot of NaN values because the original data had non numeric ranges like `0 - 20000`, `20000 - 50000`, or `100000 - 200000`. Since these are strings, they couldn’t be converted directly into numeric. To fix this, I’m going to take the average of each range and use that as a new `metric`. Hence, we’ll be able to finish dealing with the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80928c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Estimated owners  Estimated average owners\n",
      "0        0 - 20000                   10000.0\n",
      "1        0 - 20000                   10000.0\n",
      "2        0 - 20000                   10000.0\n",
      "3        0 - 20000                   10000.0\n",
      "4        0 - 20000                   10000.0\n",
      "5   50000 - 100000                   75000.0\n",
      "6        0 - 20000                   10000.0\n",
      "7        0 - 20000                   10000.0\n",
      "8        0 - 20000                   10000.0\n",
      "9   50000 - 100000                   75000.0\n"
     ]
    }
   ],
   "source": [
    "# Separate the range into two separated numeric values to calculate the average\n",
    "owners_clean = df['Estimated owners'].str.replace(' ', '', regex=False)\n",
    "\n",
    "owners_split = owners_clean.str.split('-', expand=True)\n",
    "\n",
    "owners_split[0] = pd.to_numeric(owners_split[0], errors='coerce')\n",
    "owners_split[1] = pd.to_numeric(owners_split[1], errors='coerce')\n",
    "\n",
    "df['Estimated average owners'] = (owners_split[0] + owners_split[1]) / 2\n",
    "\n",
    "print(df[['Estimated owners', 'Estimated average owners']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e955e7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned\n"
     ]
    }
   ],
   "source": [
    "# Sort the duplicates based on the priotirised columns and use them to decide which duplicates to keep\n",
    "df_sorted = df.sort_values(\n",
    "    by=['Estimated average owners', 'Positive ratio', 'Release date'],\n",
    "    ascending=[False, False, False]\n",
    ")\n",
    "\n",
    "# Keeping the first duplicate as its already sorted by the prioritised columns\n",
    "df_drop = df_sorted.drop_duplicates('Name', keep='first')\n",
    "df_drop = df_drop.drop(columns=['Average owners'])\n",
    "\n",
    "data_path_unique = script_path.parent / \"data\" / \"games_unique.csv\"\n",
    "df_drop.to_csv(data_path_unique, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f0b432",
   "metadata": {},
   "source": [
    "### Handling Encoding Issues in Game Names\n",
    "\n",
    "When first loading the dataset, some game names and metadata fields appeared with corrupted characters (e.g., `å¿èæå¤§æ2`). This was caused by reading the file with the wrong encoding. Reloading the file using `encoding=\"utf-8\"` in the initial `pd.read_csv()` resolved the issue and restored correct characters like `忍者村大战2`. No further cleaning was needed, since the original text was intact in the source file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86d3237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        AppID    Name    Developers    Publishers\n",
      "10451  754350  忍者村大战2  杭州分浪网络科技有限公司  杭州分浪网络科技有限公司\n"
     ]
    }
   ],
   "source": [
    "# Reading the new cleaned file to keep working and double checking the results from previous work\n",
    "df2 = pd.read_csv(data_path_unique, encoding=\"utf-8\", low_memory=False) \n",
    "\n",
    "appid_to_check = 754350\n",
    "print(df2[df2['AppID'] == appid_to_check][['AppID', 'Name','Developers','Publishers']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e01b5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 18)\n"
     ]
    }
   ],
   "source": [
    "duplicates2 = df2[df2.duplicated('Name', keep=False)]\n",
    "print(duplicates2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c7f40f",
   "metadata": {},
   "source": [
    "I double-checked the data in Excel, and even though the code says all rows are unique, there are still quite a few duplicates. Most of them are due to slight differences in `Name`, like upper and lower cases. So, we’re going to clean it up one more time to make sure everything is consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a9dc790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(660, 19)\n",
      "                               Name  Estimated average owners  Positive ratio  \\\n",
      "0                            Dota 2               150000000.0        0.830986   \n",
      "1                Black Myth: Wukong                75000000.0        0.958515   \n",
      "2                   Team Fortress 2                75000000.0        0.935615   \n",
      "3  Counter-Strike: Global Offensive                75000000.0        0.882611   \n",
      "4                         New World                75000000.0        0.677030   \n",
      "5               PUBG: BATTLEGROUNDS                75000000.0        0.563072   \n",
      "6                  Wallpaper Engine                35000000.0        0.980557   \n",
      "7                          Terraria                35000000.0        0.978658   \n",
      "8                     Left 4 Dead 2                35000000.0        0.974508   \n",
      "9                       Garry's Mod                35000000.0        0.965931   \n",
      "\n",
      "  Release date  \n",
      "0   2013-07-09  \n",
      "1   2024-08-19  \n",
      "2   2007-10-10  \n",
      "3   2012-08-21  \n",
      "4   2021-09-28  \n",
      "5   2017-12-21  \n",
      "6          NaN  \n",
      "7   2011-05-16  \n",
      "8   2009-11-16  \n",
      "9   2006-11-29  \n"
     ]
    }
   ],
   "source": [
    "# Making a new column with lowercased names to keep it consistent and check for duplicates\n",
    "df2['Name_lowercase'] = df2['Name'].str.lower().str.strip()\n",
    "\n",
    "duplicates2 = df2[df2['Name_lowercase'].duplicated(keep=False)]\n",
    "print(duplicates2.shape)\n",
    "\n",
    "df2_sorted = df2.sort_values(\n",
    "    by=['Estimated average owners', 'Positive ratio', 'Release date'],\n",
    "    ascending=[False, False, False]\n",
    ")\n",
    "\n",
    "print(df2_sorted[['Name','Estimated average owners','Positive ratio','Release date']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44a8628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned\n"
     ]
    }
   ],
   "source": [
    "# Dropping duplicates as well as some unnecessary columns\n",
    "df2_drop = df2_sorted.drop_duplicates('Name_lowercase', keep='first')\n",
    "df2_drop = df2_drop.drop(columns=['Name_lowercase','Estimated owners'])\n",
    "\n",
    "data_path_unique2 = script_path.parent / \"data\" / \"games_unique2.csv\"\n",
    "df2_drop.to_csv(data_path_unique2, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9174bc1",
   "metadata": {},
   "source": [
    "### Missing value\n",
    "As it has been illustrated above, some cells in the dataset contain `missing values`, which I haven’t addressed yet. This was an oversight during initial cleaning, and I plan to handle them as I continue developing the analysis. Depending on where these missing values appear, I’ll decide whether to drop them, fill them, or leave them untouched if they're irrelevant to the core questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f5cb008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AppID                           0\n",
      "Name                            1\n",
      "Release date                  127\n",
      "Required age                    0\n",
      "Price                           0\n",
      "User score                      0\n",
      "Positive                        0\n",
      "Negative                        0\n",
      "Recommendations                 0\n",
      "Average playtime forever        0\n",
      "Developers                   6438\n",
      "Publishers                   6735\n",
      "Categories                   7527\n",
      "Genres                       6412\n",
      "Tags                        36663\n",
      "Positive ratio              36827\n",
      "Estimated average owners        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Reading the newer cleaned file to keep working and double checking the results from previous work\n",
    "df3 = pd.read_csv(data_path_unique2, encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "print(df3.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964a8c62",
   "metadata": {},
   "source": [
    "Since I now have many missing values, Im thinking if some of them might have originally been presented in the duplicated rows that I deleted earlier. To check this, I’ll go back to games_cleaned.csv and investigate again with a cleaner approach using all the information i have learned throughout this process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
