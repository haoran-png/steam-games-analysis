{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b12ba3b",
   "metadata": {},
   "source": [
    "# Attemp number 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e75417",
   "metadata": {},
   "source": [
    "When i first wrote in cleaning.py I used `script_path = Path(__file__).resolve().parent` which work well in a .py as `__file__` showed the exact location of the current file. However, inside the notebook the variable `__file__` doesnt exist, to continue looking for the file path I decided to use `.cwd` without the `.parent` in the `data_path_cleaned = script_path / ... /` in notebook as this shows the same result while keeping `__file__` in cleaning.py as it is more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd70ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 19)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "script_path = Path.cwd()\n",
    "data_path_cleaned = script_path / \"data\" / \"games_cleaned.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path_cleaned, encoding=\"utf-8\")\n",
    "\n",
    "# Cheking for AppID duplicates\n",
    "duplicates = df[df.duplicated('AppID', keep=False)]\n",
    "print(duplicates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339cc9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 19)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking for Name duplicates\n",
    "duplicates = df[df.duplicated('Name', keep=False)]\n",
    "print(duplicates.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006b63fa",
   "metadata": {},
   "source": [
    "As shown earlier, all `AppID` values were unique; however, there were 2,079 duplicate entries based on `Name`. To analyze the dataset effectively, we need a clear metric to determine which duplicates to keep and which to discard. In this project, I aim to focus on user feedback while preserving enough relevance to ensure the data remains valuable. Therefore, I will use the number of `Positive` and `Negative` reviews, the `Estimated number` of owners, and the game's `Release date` as criteria for filtering duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0959c716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Average owners  Positive ratio Release date\n",
       "1019              NaN             1.0   2025-04-18\n",
       "10878             NaN             1.0   2025-04-18\n",
       "19155             NaN             1.0   2025-04-18\n",
       "19156             NaN             1.0   2025-04-18\n",
       "10879             NaN             1.0   2025-04-17\n",
       "19157             NaN             1.0   2025-04-17\n",
       "19158             NaN             1.0   2025-04-17\n",
       "19159             NaN             1.0   2025-04-17\n",
       "88616             NaN             1.0   2025-04-17\n",
       "10880             NaN             1.0   2025-04-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data types conversion\n",
    "df['Release date'] = pd.to_datetime(df['Release date'], errors='coerce')\n",
    "df['Positive'] = pd.to_numeric(df['Positive'], errors='coerce')\n",
    "df['Negative'] = pd.to_numeric(df['Negative'], errors='coerce')\n",
    "df['Average owners'] = pd.to_numeric(df['Estimated owners'], errors='coerce')\n",
    "\n",
    "df['Positive ratio'] = df['Positive'] / (df['Positive'] + df['Negative']).replace(0, np.nan)\n",
    "\n",
    "df_sorted = df.sort_values(\n",
    "    by=['Average owners', 'Positive ratio', 'Release date'],\n",
    "    ascending=[False, False, False]\n",
    ")\n",
    "\n",
    "print(df_sorted[['Average owners', 'Positive ratio', 'Release date']].head(10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c2193",
   "metadata": {},
   "source": [
    "The `Estimated owners` column was showing a lot of NaN values because the original data had non numeric ranges like `0 - 20000`, `20000 - 50000`, or `100000 - 200000`. Since these are strings, they couldn’t be converted directly into numeric. To fix this, I’m going to take the average of each range and use that as a new `metric`. Hence, we’ll be able to finish dealing with the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80928c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Estimated owners  Estimated average owners\n",
       "0        0 - 20000                   10000.0\n",
       "1        0 - 20000                   10000.0\n",
       "2        0 - 20000                   10000.0\n",
       "3        0 - 20000                   10000.0\n",
       "4  200000 - 500000                   10000.0\n",
       "5        0 - 20000                   75000.0\n",
       "6        0 - 20000                   10000.0\n",
       "7        0 - 20000                   10000.0\n",
       "8        0 - 20000                   10000.0\n",
       "9   50000 - 100000                   75000.0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Separate the range into two separated numeric values to calculate the average\n",
    "owners_clean = owners_clean.str.replace(' ', '', regex=False)\n",
    "\n",
    "owners_split = owners_clean.str.split('-', expand=True)\n",
    "\n",
    "owners_split[0] = pd.to_numeric(owners_split[0], errors='coerce')\n",
    "owners_split[1] = pd.to_numeric(owners_split[1], errors='coerce')\n",
    "\n",
    "df['Estimated average owners'] = (owners_split[0] + owners_split[1]) / 2\n",
    "\n",
    "print(df[['Estimated owners', 'Estimated average owners']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e955e7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleaned\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sort the duplicates based on the priotirised columns and use them to decide which duplicates to keep\n",
    "df_sorted = df.sort_values(\n",
    "    by=['Estimated average owners', 'Positive ratio', 'Release date'],\n",
    "    ascending=[False, False, False]\n",
    ")\n",
    "\n",
    "# Keeping the first duplicate as its already sorted by the prioritised columns\n",
    "df_drop = df_sorted.drop_duplicates('Name', keep='first')\n",
    "df_drop.to_csv(data_path_cleaned, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f0b432",
   "metadata": {},
   "source": [
    "### Handling Encoding Issues in Game Names\n",
    "\n",
    "When first loading the dataset, some game names and metadata fields appeared with corrupted characters (e.g., `å¿èæå¤§æ2`). This was caused by reading the file with the wrong encoding. Reloading the file using `encoding=\"utf-8\"` in the initial `pd.read_csv()` resolved the issue and restored correct characters like `忍者村大战2`. No further cleaning was needed, since the original text was intact in the source file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86d3237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        AppID    Name    Developers    Publishers\n",
       "10251  754350  忍者村大战2  杭州分浪网络科技有限公司  杭州分浪网络科技有限公司\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading the new cleaned file to keep working and double checking the results from previous work\n",
    "data_path_unique = script_path / \"data\" / \"games_cleaned_unique.csv\"\n",
    "df2 = pd.read_csv(data_path_unique, encoding=\"utf-8\") \n",
    "\n",
    "appid_to_check = 754350\n",
    "print(df2[df2['AppID'] == appid_to_check][['AppID', 'Name','Developers','Publishers']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e01b5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 19)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "duplicates2 = df2[df2.duplicated('Name', keep=False)]\n",
    "print(duplicates2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c7f40f",
   "metadata": {},
   "source": [
    "I double-checked the data in Excel, and even though the code says all rows are unique, there are still quite a few duplicates. Most of them are due to slight differences in `Name`, like upper and lower cases. So, we’re going to clean it up one more time to make sure everything is consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9dc790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(660, 20)\n",
       "                               Name  ...  Release date\n",
       "0                            Dota 2  ...    2013-07-09\n",
       "1                     Dungeon Diver  ...    2024-07-19\n",
       "2                   Team Fortress 2  ...    2007-10-10\n",
       "3  Counter-Strike: Global Offensive  ...    2012-08-21\n",
       "4                         New World  ...    2021-09-28\n",
       "5               PUBG: BATTLEGROUNDS  ...    2017-12-21\n",
       "6                  Wallpaper Engine  ...           NaN\n",
       "7                          Terraria  ...    2011-05-16\n",
       "8                     Left 4 Dead 2  ...    2009-11-16\n",
       "9                       Garry's Mod  ...    2006-11-29\n",
       "\n",
       "[10 rows x 4 columns]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Making a new column with lowercased names to keep it consistent and check for duplicates\n",
    "df2['Name_lowercase'] = df2['Name'].str.lower().str.strip()\n",
    "\n",
    "duplicates2 = df2[df2['Name_lowercase'].duplicated(keep=False)]\n",
    "print(duplicates2.shape)\n",
    "\n",
    "df2_sorted = df2.sort_values(\n",
    "    by=['Estimated average owners', 'Positive ratio', 'Release date'],\n",
    "    ascending=[False, False, False]\n",
    ")\n",
    "\n",
    "print(df2_sorted[['Name','Estimated average owners','Positive ratio','Release date']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8628a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleaned\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dropping duplicates as well as some unnecessary columns\n",
    "df2_drop = df2_sorted.drop_duplicates('Name_lowercase', keep='first')\n",
    "df2_drop = df2_drop.drop(columns=['Name_lowercase','Estimated owners'])\n",
    "df2_drop.to_csv(data_path_unique, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9174bc1",
   "metadata": {},
   "source": [
    "### Missing value\n",
    "As it has been illustrated above, some cells in the dataset contain `missing values`, which I haven’t addressed yet. This was an oversight during initial cleaning, and I plan to handle them as I continue developing the analysis. Depending on where these missing values appear, I’ll decide whether to drop them, fill them, or leave them untouched if they're irrelevant to the core questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5cb008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AppID                            0\n",
       "Name                             1\n",
       "Release date                   127\n",
       "Required age                     0\n",
       "Price                            0\n",
       "User score                       0\n",
       "Positive                         0\n",
       "Negative                         0\n",
       "Recommendations                  0\n",
       "Average playtime forever         0\n",
       "Developers                    6440\n",
       "Publishers                    6737\n",
       "Categories                    7529\n",
       "Genres                        6412\n",
       "Tags                         36695\n",
       "Average owners              109991\n",
       "Positive ratio               36858\n",
       "Estimated average owners        22\n",
       "dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path_unique2 = script_path / \"data\" / \"games_cleaned_unique2.csv\"\n",
    "\n",
    "# Reading the newer cleaned file to keep working and double checking the results from previous work\n",
    "df3 = pd.read_csv(data_path_unique2, encoding=\"utf-8\")\n",
    "\n",
    "print(df3.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964a8c62",
   "metadata": {},
   "source": [
    "Since I now have many missing values, Im thinking if some of them might have originally been presented in the duplicated rows that I deleted earlier. To check this, I’ll go back to games_cleaned.csv and investigate again with a cleaner approach using all the information i have learned throughout this process"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
